# Llama Gpt é¡¹ç›®çš„ç®€å•ä»‹ç»ä»¥åŠåˆ‡æ¢CUDAçš„æ–¹æ³•

llama-gpt æ˜¯ä¸€ä¸ª éœ€è¦è‡ªå·±æ¶è®¾ï¼Œç¦»çº¿ï¼Œç±»ä¼¼ChatGPTçš„èŠå¤©æœºå™¨äººï¼Œç”±Llama 2 æä¾›æ”¯æŒã€‚

å®˜æ–¹ github åœ°å€ï¼š https://github.com/getumbrel/llama-gpt.git

å®˜æ–¹ç®€ä»‹ï¼š  
> A self-hosted, offline, ChatGPT-like chatbot. Powered by Llama 2. 100% private, with no data leaving your device.

## ä¸»è¦å†…å®¹

### ä½“éªŒ 
è¿™æ˜¯ç¬”è€…ç¬¬ä¸€æ¬¡ä½“éªŒgptç±»çš„å·¥å…·ï¼Œ ä¹‹å‰å¹¶æ²¡æœ‰å°è¯•è¿‡chatGPTï¼Œ æ„Ÿè§‰è¿™ä¸ªllama-gpt å‹‰å¼ºå¯ç”¨ï¼Œ ä¸è¿‡æ›´å¤šçš„è¿˜æ˜¯æœŸå¾…å®ƒæœªæ¥çš„è¡¨ç°ã€‚

- ä»“åº“çš„ç‰ˆæœ¬æ˜¯ çº¯CPU è¿ç®—çš„ï¼Œ æ‰€ä»¥é€Ÿåº¦å¾ˆæ…¢ã€‚ ç¬”è€…çš„U æ˜¯amd 5600, ä¸€ä¸ªé—®é¢˜éœ€è¦å‡ åˆ†é’Ÿæ‰èƒ½å¼€å§‹å“åº”ï¼Œ åˆ°å“åº”ç»“æŸè¿˜éœ€è¦ä¸€æ®µæ—¶é—´ã€‚ *2023å¹´8æœˆ21æ—¥*
- ä¸è¿‡å¯ä»¥è‡ªå·±åˆ‡æ¢åˆ°Nå¡çš„ CUDA ä¸Šè¿›è¡Œè¿ç®—ï¼Œ æœ‰ç‚¹éº»çƒ¦ï¼Œ ä½†æ˜¯å¯ä»¥åˆ‡æ¢ã€‚
- å“åº”çš„å†…å®¹ æ²¡æœ‰æ’ç‰ˆï¼Œè¿™ä¸ªå¾ˆéš¾å—ï¼Œå¹¶ä¸”ç¬”è€…ç›®å‰æ²¡æœ‰æ‰¾åˆ°åŠæ³•å»è§£å†³ã€‚   *åç»­æ›´æ–°å¯è§ï¼š https://github.com/getumbrel/llama-gpt/issues/28*
- å¯ä»¥ç”¨ä¸­æ–‡æé—®ï¼Œ ä½†æ˜¯é»˜è®¤æƒ…å†µä¸‹ä¼šå›å¤è‹±æ–‡
- ä¼šæ— è§†ä¸€äº›è¾“å…¥æç¤ºè¯ï¼Œ æ¯”å¦‚ æƒ³è®©å®ƒçœç•¥è§£é‡Šè¯´æ˜éƒ¨åˆ†ï¼Œ ä½†æ˜¯æ²¡ç”¨ï¼Œå®ƒä¸€å®šä¼šç»™ä½ è§£é‡Šä¸€ä¸‹ã€‚

ä¸‹é¢çœ‹å‡ å¼ å›¾ã€‚ 

![llama-gpt1](/img/periphery/llama-gpt1.png)

è™½ç„¶æ’ç‰ˆå¾ˆéš¾å—ï¼Œ ä½†æ˜¯ç”Ÿæˆå‡ºç°çš„ä¸œè¥¿ï¼Œ è¿˜æ˜¯å¯ä»¥è€ƒè™‘çš„ã€‚ 

```cpp 
#include <iostream>

#include "imgui.h"

int main() {
  ImGui::CreateContext();
  ImGui::StyleColor( & ImGuiCol_Text, ImVec4(1.0 f, 1.0 f, 1.0 f, 1.0 f));
  ImGui::Begin("Form", nullptr);
  ImGui::Text("Name:");
  ImGui::SameLine();
  ImGui::InputText("name", name, IM_ARRAYSIZE(name));
  ImGui::Text("Age:");
  ImGui::SameLine();
  ImGui::InputText("age", age, IM_ARRAYSIZE(age));
  ImGui::Button("Submit");
  ImGui::Button("Reset");
  ImGui::End();
  ImGui::DestroyContext();
  return 0;
}
```
*ä½¿ç”¨ https://codebeautify.org/cpp-formatter-beautifier è‡ªåŠ¨æ ¼å¼åŒ–*

```cpp
#include <iostream>

#include "imgui.h"

int main() {
  ImGui::CreateContext();
  ImGui::StyleColor( & ImGuiCol_Text, ImVec4(1.0 f, 1.0 f, 1.0 f, 1.0 f));
  ImGui::Begin("Form", nullptr);
  ImGui::Text("Name:");
  ImGui::SameLine();
  ImGui::InputText("name", name, IM_ARRAYSIZE(name));
  ImGui::Text("Age:");
  ImGui::SameLine();
  ImGui::InputInt("age", age);
  if (ImGui::Button("Submit")) {
    std::cout << "Submit button pressed!" << std::endl;
  }
  if (ImGui::Button("Reset")) {
    name[0] = '\0';
    age = 0;
  }
  ImGui::End();
  ImGui::DestroyContext();
  return 0;
}
```

ç¬¬äºŒç‰ˆä»£ç æ¯”ç¬¬ä¸€ç‰ˆæœ¬å¤šäº†ä¸€äº›å†…å®¹ï¼Œ ä½†æ˜¯ä»ç„¶æ²¡å¸®æˆ‘å£°æ˜ name, age ä¸¤ä¸ªå˜é‡ ğŸ˜†


å¶å°”çš„æƒ…å†µä¸‹ï¼Œ ä¼šç»™ä¸€ä¸ªå¾ˆå¥½çš„æ’ç‰ˆã€‚ 

![llama-gpt2](/img/periphery/llama-gpt2.png)


å¯ä»¥ç”Ÿæˆ unityçš„è„šæœ¬, è¿™é‡Œè¿˜å¯ä»¥çœ‹åˆ°æ˜¯å­˜åœ¨ä¸Šä¸‹æ–‡è¯­å¢ƒçš„ã€‚  ~~ä½†æ˜¯æˆ‘çœ‹äº†ä¸€ä¸‹æ—¥å¿—ï¼Œ æ„Ÿè§‰å°±æ˜¯æŠŠèŠå¤©è®°å½•éƒ½å‘äº†ä¸€éè¿‡å»ã€‚ã€‚~~

![llama-gpt3](/img/periphery/llama-gpt3.png)

### åº”ç”¨

- å°æ®µä»£ç ç”Ÿæˆ
- ä¸€äº›å¸¸è§çš„ç®—æ³•ä»£ç ç”Ÿæˆ
  - ç”Ÿæˆçš„å†’æ³¡æ’åºå¯ä»¥ä½¿ç”¨ï¼Œ ä½†æ˜¯ å¿«é€Ÿæ’åºå´æœ‰è¯­æ³•é—®é¢˜ï¼Œ ä¸”æ²¡ç»™æˆ‘è§£å†³ã€‚ã€‚
- ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡
  - ä½†æ˜¯æ— æ³•ä»è‹±æ–‡ç¿»è¯‘ä¸­æ–‡
- é—®ä¸€äº›ä¹±ä¸ƒå…«ç³Ÿçš„
- ç”Ÿæˆ  CURD ç›¸å…³çš„ä»£ç ã€‚ 


### åˆ‡æ¢éƒ¨åˆ†å†…å®¹åˆ°GPUä¸Šè¿è¡Œ

æœ¬æ–‡æè¿°çš„æ–¹æ³•æ˜¯ä½¿ç”¨ Nå¡çš„CUDA è¿›è¡Œè¿ç®—ã€‚  æ›´æ¢ä¹‹åï¼Œ å¤§å¤šæ•°é—®é¢˜åœ¨å‡ ç§’å†…å°±å¯ä»¥å¼€å§‹å›ç­”äº†ã€‚

*è¿™åªæ˜¯ä¸€ä¸ªä¸´æ—¶çš„åŠæ³•ï¼Œ å®˜æ–¹å¯èƒ½ä¼šåœ¨ä¸ä¹…çš„å°†æ¥æ”¯æŒ CUDAã€‚ å¯è§ https://github.com/getumbrel/llama-gpt/issues/6*


*ç¬”è€…åœ¨ Debian 11 ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œ æˆåŠŸè¿è¡Œäº† 13Bæ¨¡å‹ã€‚*

*æ›´å¤šçš„è¯´æ˜åœ¨æœ€åéƒ¨åˆ†*

**ç®€å•ç‰ˆæœ¬**
- ç¼–è¯‘ llama-cpp-python çš„ cuda ç‰ˆæœ¬ é•œåƒ
- æ‰‹åŠ¨æŠŠæ¨¡å‹æ˜ å°„åˆ°å®¹å™¨ä¸­
- è°ƒæ•´å¹¶ä½¿ç”¨ `llama-gpt/api/run.sh` æ–‡ä»¶æ¥å¯åŠ¨

**é•¿ç‰ˆæœ¬**

1. å®‰è£… å¹¶ä¸”è®¾ç½®å¥½ dockerï¼Œ åŒ…æ‹¬ [nvidia container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
2. git clone https://github.com/abetlen/llama-cpp-python.git
3. å‡†å¤‡å¥½ æ¨¡å‹
   1. llama-2-7b-chat.bin - https://huggingface.co/TheBloke/Nous-Hermes-Llama-2-7B-GGML/resolve/main/nous-hermes-llama-2-7b.ggmlv3.q4_0.bin
   2. llama-2-13b-chat.bin - https://huggingface.co/TheBloke/Nous-Hermes-Llama2-GGML/resolve/main/nous-hermes-llama2-13b.ggmlv3.q4_0.bin
   3. æˆ‘è§‰å¾— 7b æˆ–è€… 13b çš„å°±è¶³å¤Ÿäº†
   4. ä¸‹è½½å¥½çš„æ¨¡å‹æ”¾åˆ°å®¿ä¸»æœºçš„æŸä¸ªæ–‡ä»¶å¤¹ï¼Œ æ¯”å¦‚ï¼š `/home/debian/docker/llama-models`
   5. æˆ‘ä½¿ç”¨çš„æ˜¯ `llama-2-13b-chat.bin` è¿™ä¸ªæ¨¡å‹
4. æŸ¥é˜…æ–‡æ¡£ï¼š  https://github.com/abetlen/llama-cpp-python/tree/main/docker#cuda_simple
   1. ä¸éœ€è¦æ‰§è¡Œæœ€åçš„ docker run å‘½ä»¤ï¼Œ ä½¿ç”¨ `docker build -t cuda_simple .` æ„å»ºå¥½é•œåƒå³å¯
   2. ä¿®æ”¹ç‰ˆæœ¬çš„ Dockerfile: https://github.com/Aincvy/llama-cpp-python/blob/dockerfile-cn-mirrors/docker/cuda_simple/Dockerfile-CN
      1. ä½¿ç”¨äº† é˜¿é‡Œçš„ apt æº
      2. ä½¿ç”¨äº† æ¸…åå¤§å­¦çš„ python æº
5. æ‰§è¡Œå‘½ä»¤ `sudo docker tag cuda_simple llama-cpp-python-cuda:latest`  
6. å¯»æ‰¾ä¸€ä¸ªç›®å½•ä½œä¸º docker compose çš„ç›®å½•ï¼Œ æ¯”å¦‚ï¼š `/home/debian/docker-compose/llama-gpt-cuda`
7. æ–°å»ºæ–‡ä»¶ `docker-compose.yaml` ,å¹¶å¡«å……ä¸‹é¢çš„å†…å®¹
```yaml
version: '3.6'

services:
  llama-gpt-api-13b:
    # image: 'ghcr.io/getumbrel/llama-gpt-api-llama-2-13b-chat:latest'
    image: 'llama-cpp-python-cuda:latest'
    # build:
    #   context: ./api
    #   dockerfile: 13B.Dockerfile
    restart: on-failure
    environment:
      MODEL: '/models/llama-2-13b-chat.bin'
      USE_MLOCK: 1
    volumes:
      - '/home/debian/docker/llama-models:/models/'
      - './data:/home'
    cap_add:
      - IPC_LOCK
      - SYS_RESOURCE
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: /bin/sh /home/run.sh

  llama-gpt-ui:
    image: 'getumbrel/llama-gpt-ui:latest'
    container_name: 'llama-gpt-ui'
    ports: 
      - 3000:3000
    restart: on-failure
    environment:
      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'
      - 'OPENAI_API_HOST=http://llama-gpt-api-13b:8000'
      - 'DEFAULT_MODEL=/models/llama-2-13b-chat.bin'
      - 'WAIT_HOSTS=llama-gpt-api-13b:8000'
      - 'WAIT_TIMEOUT=600'

```
8. åˆ›å»ºä¸€ä¸ªç›®å½•ï¼š `data` 
9. åˆ›å»ºä¸€ä¸ªæ–‡ä»¶ï¼š `data/run.sh` å¹¶å¡«å……ä¸‹é¢çš„å†…å®¹
*ä» llama-gpt/api/run.sh å¤åˆ¶ï¼Œç„¶åè°ƒæ•´çš„*
```shell
#!/bin/bash

# make build

# Get the number of available threads on the system
n_threads=$(grep -c ^processor /proc/cpuinfo)

# Define context window
n_ctx=4096

# Offload everything to CPU
n_gpu_layers=37      

# Define batch size
n_batch=2096
# If total RAM is less than 8GB, set batch size to 1024
total_ram=$(cat /proc/meminfo | grep MemTotal | awk '{print $2}')
if [ $total_ram -lt 8000000 ]; then
    n_batch=1024
fi

echo "Initializing server with:"
echo "Batch size: $n_batch"
echo "Number of CPU threads: $n_threads"
echo "Number of GPU layers: $n_gpu_layers"
echo "Context window: $n_ctx"

exec python3 -m llama_cpp.server --n_ctx $n_ctx --n_threads $n_threads --n_gpu_layers $n_gpu_layers --n_batch $n_batch
```  
   
10. è°ƒæ•´ `n_gpu_layers`å‚æ•°ï¼Œ è¿‡å¤§å¯èƒ½ä¼šå¯¼è‡´ OOMï¼Œ æˆ‘æ˜¯3060 12Gï¼Œ 37å·²ç»éå¸¸æé™äº†ã€‚
11. `sudo docker compose up`  æŸ¥çœ‹èƒ½å¦æ­£å¸¸è¿è¡Œï¼Œ å¦‚æœæ²¡æœ‰ä»€ä¹ˆé—®é¢˜çš„è¯ï¼Œ åˆ™ä½¿ç”¨ `sudo docker compose up -d ` è¿›å…¥åå°è¿è¡Œã€‚


å…¶ä»–è¯´æ˜
- GPU çš„å ç”¨å¥½åƒæ²¡æœ‰è·‘æ»¡ï¼Œ å¤§çº¦åœ¨ 30% é™„è¿‘ã€‚  CPUè¿˜æ˜¯100% åœ¨è¿è¡Œï¼Œ 
- é€Ÿåº¦ç¡®å®æå‡äº†ä¸å°‘ï¼Œ ~~ä½†æ˜¯æ²¡æœ‰åˆ°è¾¾ç«‹å³å“åº”çš„åœ°æ­¥ã€‚~~  å¾ˆå¤šé—®é¢˜å¯ä»¥åœ¨å‡ ç§’å†…å¼€å§‹å›ç­”ã€‚
- èŠå¤©è®°å½•è¶Šå¤šï¼Œ è¶Šå®¹æ˜“ å´©æºƒï¼Œ æš‚æ—¶æ²¡çœ‹åˆ°å´©æºƒåŸå› ï¼Œåªçœ‹åˆ°äº† exit code 0
  - å¯èƒ½å’Œdocker å®¹å™¨å…³é—­å†å¯åŠ¨ä¹Ÿæœ‰ç‚¹å…³ç³»æŠŠã€‚
- æ¨¡å‹çš„è·¯å¾„å¥½åƒåªèƒ½ç”¨ `/models/llama-2-13b-chat.bin` ï¼Œæˆ–è€…åªèƒ½æ˜¯`/models/` ç›®å½•ï¼Œ å¦åˆ™ UI ç¨‹åºä¼šå‡ºç°é—®é¢˜ã€‚



## å…¶ä»–

å°è¯•äº†ä¸€ä¸‹  https://github.com/oobabooga/text-generation-webui ï¼Œ åŒæ ·å®‰è£…æœ‰ç‚¹éº»çƒ¦ï¼Œ ä½†æ˜¯è¿™ä¸ªæ’ç‰ˆå¾ˆå¥½ã€‚ *ä½¿ç”¨çš„åŒä¸€ä¸ªæ¨¡å‹*

---

> ä½œè€…: Aincvy  
> URL: https://fantasyplayer.link/periphery/llama-gpt-%E9%A1%B9%E7%9B%AE%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%88%87%E6%8D%A2cuda%E7%9A%84%E6%96%B9%E6%B3%95/  

