# Llama Gpt é¡¹ç›®çš„ç®€å•ä»‹ç»ä»¥åŠåˆ‡æ¢CUDAçš„æ–¹æ³•

llama-gpt æ˜¯ä¸€ä¸ª éœ€è¦è‡ªå·±æž¶è®¾ï¼Œç¦»çº¿ï¼Œç±»ä¼¼ChatGPTçš„èŠå¤©æœºå™¨äººï¼Œç”±Llama 2 æä¾›æ”¯æŒã€‚

å®˜æ–¹ github åœ°å€ï¼š https://github.com/getumbrel/llama-gpt.git

å®˜æ–¹ç®€ä»‹ï¼š  
&gt; A self-hosted, offline, ChatGPT-like chatbot. Powered by Llama 2. 100% private, with no data leaving your device.

## ä¸»è¦å†…å®¹

### ä½“éªŒ 
è¿™æ˜¯ç¬”è€…ç¬¬ä¸€æ¬¡ä½“éªŒgptç±»çš„å·¥å…·ï¼Œ ä¹‹å‰å¹¶æ²¡æœ‰å°è¯•è¿‡chatGPTï¼Œ æ„Ÿè§‰è¿™ä¸ªllama-gpt å‹‰å¼ºå¯ç”¨ï¼Œ ä¸è¿‡æ›´å¤šçš„è¿˜æ˜¯æœŸå¾…å®ƒæœªæ¥çš„è¡¨çŽ°ã€‚

- ä»“åº“çš„ç‰ˆæœ¬æ˜¯ çº¯CPU è¿ç®—çš„ï¼Œ æ‰€ä»¥é€Ÿåº¦å¾ˆæ…¢ã€‚ ç¬”è€…çš„U æ˜¯amd 5600, ä¸€ä¸ªé—®é¢˜éœ€è¦å‡ åˆ†é’Ÿæ‰èƒ½å¼€å§‹å“åº”ï¼Œ åˆ°å“åº”ç»“æŸè¿˜éœ€è¦ä¸€æ®µæ—¶é—´ã€‚ *2023å¹´8æœˆ21æ—¥*
- ä¸è¿‡å¯ä»¥è‡ªå·±åˆ‡æ¢åˆ°Nå¡çš„ CUDA ä¸Šè¿›è¡Œè¿ç®—ï¼Œ æœ‰ç‚¹éº»çƒ¦ï¼Œ ä½†æ˜¯å¯ä»¥åˆ‡æ¢ã€‚
- å“åº”çš„å†…å®¹ æ²¡æœ‰æŽ’ç‰ˆï¼Œè¿™ä¸ªå¾ˆéš¾å—ï¼Œå¹¶ä¸”ç¬”è€…ç›®å‰æ²¡æœ‰æ‰¾åˆ°åŠžæ³•åŽ»è§£å†³ã€‚   *åŽç»­æ›´æ–°å¯è§ï¼š https://github.com/getumbrel/llama-gpt/issues/28*
- å¯ä»¥ç”¨ä¸­æ–‡æé—®ï¼Œ ä½†æ˜¯é»˜è®¤æƒ…å†µä¸‹ä¼šå›žå¤è‹±æ–‡
- ä¼šæ— è§†ä¸€äº›è¾“å…¥æç¤ºè¯ï¼Œ æ¯”å¦‚ æƒ³è®©å®ƒçœç•¥è§£é‡Šè¯´æ˜Žéƒ¨åˆ†ï¼Œ ä½†æ˜¯æ²¡ç”¨ï¼Œå®ƒä¸€å®šä¼šç»™ä½ è§£é‡Šä¸€ä¸‹ã€‚

ä¸‹é¢çœ‹å‡ å¼ å›¾ã€‚ 

![llama-gpt1](/img/periphery/llama-gpt1.png)

è™½ç„¶æŽ’ç‰ˆå¾ˆéš¾å—ï¼Œ ä½†æ˜¯ç”Ÿæˆå‡ºçŽ°çš„ä¸œè¥¿ï¼Œ è¿˜æ˜¯å¯ä»¥è€ƒè™‘çš„ã€‚ 

```cpp 
#include &lt;iostream&gt;

#include &#34;imgui.h&#34;

int main() {
  ImGui::CreateContext();
  ImGui::StyleColor( &amp; ImGuiCol_Text, ImVec4(1.0 f, 1.0 f, 1.0 f, 1.0 f));
  ImGui::Begin(&#34;Form&#34;, nullptr);
  ImGui::Text(&#34;Name:&#34;);
  ImGui::SameLine();
  ImGui::InputText(&#34;name&#34;, name, IM_ARRAYSIZE(name));
  ImGui::Text(&#34;Age:&#34;);
  ImGui::SameLine();
  ImGui::InputText(&#34;age&#34;, age, IM_ARRAYSIZE(age));
  ImGui::Button(&#34;Submit&#34;);
  ImGui::Button(&#34;Reset&#34;);
  ImGui::End();
  ImGui::DestroyContext();
  return 0;
}
```
*ä½¿ç”¨ https://codebeautify.org/cpp-formatter-beautifier è‡ªåŠ¨æ ¼å¼åŒ–*

```cpp
#include &lt;iostream&gt;

#include &#34;imgui.h&#34;

int main() {
  ImGui::CreateContext();
  ImGui::StyleColor( &amp; ImGuiCol_Text, ImVec4(1.0 f, 1.0 f, 1.0 f, 1.0 f));
  ImGui::Begin(&#34;Form&#34;, nullptr);
  ImGui::Text(&#34;Name:&#34;);
  ImGui::SameLine();
  ImGui::InputText(&#34;name&#34;, name, IM_ARRAYSIZE(name));
  ImGui::Text(&#34;Age:&#34;);
  ImGui::SameLine();
  ImGui::InputInt(&#34;age&#34;, age);
  if (ImGui::Button(&#34;Submit&#34;)) {
    std::cout &lt;&lt; &#34;Submit button pressed!&#34; &lt;&lt; std::endl;
  }
  if (ImGui::Button(&#34;Reset&#34;)) {
    name[0] = &#39;\0&#39;;
    age = 0;
  }
  ImGui::End();
  ImGui::DestroyContext();
  return 0;
}
```

ç¬¬äºŒç‰ˆä»£ç æ¯”ç¬¬ä¸€ç‰ˆæœ¬å¤šäº†ä¸€äº›å†…å®¹ï¼Œ ä½†æ˜¯ä»ç„¶æ²¡å¸®æˆ‘å£°æ˜Ž name, age ä¸¤ä¸ªå˜é‡ ðŸ˜†


å¶å°”çš„æƒ…å†µä¸‹ï¼Œ ä¼šç»™ä¸€ä¸ªå¾ˆå¥½çš„æŽ’ç‰ˆã€‚ 

![llama-gpt2](/img/periphery/llama-gpt2.png)


å¯ä»¥ç”Ÿæˆ unityçš„è„šæœ¬, è¿™é‡Œè¿˜å¯ä»¥çœ‹åˆ°æ˜¯å­˜åœ¨ä¸Šä¸‹æ–‡è¯­å¢ƒçš„ã€‚  *ä½†æ˜¯æˆ‘çœ‹äº†ä¸€ä¸‹æ—¥å¿—ï¼Œ æ„Ÿè§‰å°±æ˜¯æŠŠèŠå¤©è®°å½•éƒ½å‘äº†ä¸€éè¿‡åŽ»ã€‚ã€‚ å¥½åƒopenai ä¹Ÿæ˜¯è¿™ä¹ˆå¤„ç†çš„*

![llama-gpt3](/img/periphery/llama-gpt3.png)

### åº”ç”¨

- å°æ®µä»£ç ç”Ÿæˆ
- ä¸€äº›å¸¸è§çš„ç®—æ³•ä»£ç ç”Ÿæˆ
  - ç”Ÿæˆçš„å†’æ³¡æŽ’åºå¯ä»¥ä½¿ç”¨ï¼Œ ä½†æ˜¯ å¿«é€ŸæŽ’åºå´æœ‰è¯­æ³•é—®é¢˜ï¼Œ ä¸”æ²¡ç»™æˆ‘è§£å†³ã€‚ã€‚
- ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡
  - ä½†æ˜¯æ— æ³•ä»Žè‹±æ–‡ç¿»è¯‘ä¸­æ–‡
- é—®ä¸€äº›ä¹±ä¸ƒå…«ç³Ÿçš„
- ç”Ÿæˆ  CURD ç›¸å…³çš„ä»£ç ã€‚ 


### åˆ‡æ¢éƒ¨åˆ†å†…å®¹åˆ°GPUä¸Šè¿è¡Œ

æœ¬æ–‡æè¿°çš„æ–¹æ³•æ˜¯ä½¿ç”¨ Nå¡çš„CUDA è¿›è¡Œè¿ç®—ã€‚  æ›´æ¢ä¹‹åŽï¼Œ å¤§å¤šæ•°é—®é¢˜åœ¨å‡ ç§’å†…å°±å¯ä»¥å¼€å§‹å›žç­”äº†ã€‚

*è¿™åªæ˜¯ä¸€ä¸ªä¸´æ—¶çš„åŠžæ³•ï¼Œ å®˜æ–¹å¯èƒ½ä¼šåœ¨ä¸ä¹…çš„å°†æ¥æ”¯æŒ CUDAã€‚ å¯è§ https://github.com/getumbrel/llama-gpt/issues/6*


*ç¬”è€…åœ¨ Debian 11 ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œ æˆåŠŸè¿è¡Œäº† 13Bæ¨¡åž‹ã€‚*

*æ›´å¤šçš„è¯´æ˜Žåœ¨æœ€åŽéƒ¨åˆ†*

**ç®€å•ç‰ˆæœ¬**
- ç¼–è¯‘ llama-cpp-python çš„ cuda ç‰ˆæœ¬ é•œåƒ
- æ‰‹åŠ¨æŠŠæ¨¡åž‹æ˜ å°„åˆ°å®¹å™¨ä¸­
- è°ƒæ•´å¹¶ä½¿ç”¨ `llama-gpt/api/run.sh` æ–‡ä»¶æ¥å¯åŠ¨

**é•¿ç‰ˆæœ¬**

1. å®‰è£… å¹¶ä¸”è®¾ç½®å¥½ dockerï¼Œ åŒ…æ‹¬ [nvidia container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
2. git clone https://github.com/abetlen/llama-cpp-python.git
3. å‡†å¤‡å¥½ æ¨¡åž‹
   1. llama-2-7b-chat.bin - https://huggingface.co/TheBloke/Nous-Hermes-Llama-2-7B-GGML/resolve/main/nous-hermes-llama-2-7b.ggmlv3.q4_0.bin
   2. llama-2-13b-chat.bin - https://huggingface.co/TheBloke/Nous-Hermes-Llama2-GGML/resolve/main/nous-hermes-llama2-13b.ggmlv3.q4_0.bin
   3. æˆ‘è§‰å¾— 7b æˆ–è€… 13b çš„å°±è¶³å¤Ÿäº†
      1. ç»è¿‡å°è¯•ï¼Œ 7b q4 ç‰ˆæœ¬ è´¨é‡è¾ƒå·®ï¼Œ å»ºè®®ä½¿ç”¨13Bç‰ˆæœ¬ã€‚
   4. ä¸‹è½½å¥½çš„æ¨¡åž‹æ”¾åˆ°å®¿ä¸»æœºçš„æŸä¸ªæ–‡ä»¶å¤¹ï¼Œ æ¯”å¦‚ï¼š `/home/debian/docker/llama-models`
   5. æˆ‘ä½¿ç”¨çš„æ˜¯ `llama-2-13b-chat.bin` è¿™ä¸ªæ¨¡åž‹
4. æŸ¥é˜…æ–‡æ¡£ï¼š  https://github.com/abetlen/llama-cpp-python/tree/main/docker#cuda_simple
   1. ä¸éœ€è¦æ‰§è¡Œæœ€åŽçš„ docker run å‘½ä»¤ï¼Œ ä½¿ç”¨ `docker build -t cuda_simple .` æž„å»ºå¥½é•œåƒå³å¯
   2. ä¿®æ”¹ç‰ˆæœ¬çš„ Dockerfile: https://github.com/Aincvy/llama-cpp-python/blob/dockerfile-cn-mirrors/docker/cuda_simple/Dockerfile-CN
      1. ä½¿ç”¨äº† é˜¿é‡Œçš„ apt æº
      2. ä½¿ç”¨äº† æ¸…åŽå¤§å­¦çš„ python æº
5. æ‰§è¡Œå‘½ä»¤ `sudo docker tag cuda_simple llama-cpp-python-cuda:latest`  
6. å¯»æ‰¾ä¸€ä¸ªç›®å½•ä½œä¸º docker compose çš„ç›®å½•ï¼Œ æ¯”å¦‚ï¼š `/home/debian/docker-compose/llama-gpt-cuda`
7. æ–°å»ºæ–‡ä»¶ `docker-compose.yaml` ,å¹¶å¡«å……ä¸‹é¢çš„å†…å®¹
```yaml
version: &#39;3.6&#39;

services:
  llama-gpt-api-13b:
    # image: &#39;ghcr.io/getumbrel/llama-gpt-api-llama-2-13b-chat:latest&#39;
    image: &#39;llama-cpp-python-cuda:latest&#39;
    # build:
    #   context: ./api
    #   dockerfile: 13B.Dockerfile
    restart: on-failure
    environment:
      MODEL: &#39;/models/llama-2-13b-chat.bin&#39;
      USE_MLOCK: 1
    volumes:
      - &#39;/home/debian/docker/llama-models:/models/&#39;
      - &#39;./data:/home&#39;
    cap_add:
      - IPC_LOCK
      - SYS_RESOURCE
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: /bin/sh /home/run.sh

  llama-gpt-ui:
    image: &#39;getumbrel/llama-gpt-ui:latest&#39;
    container_name: &#39;llama-gpt-ui&#39;
    ports: 
      - 3000:3000
    restart: on-failure
    environment:
      - &#39;OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX&#39;
      - &#39;OPENAI_API_HOST=http://llama-gpt-api-13b:8000&#39;
      - &#39;DEFAULT_MODEL=/models/llama-2-13b-chat.bin&#39;
      - &#39;WAIT_HOSTS=llama-gpt-api-13b:8000&#39;
      - &#39;WAIT_TIMEOUT=600&#39;

```
8. åˆ›å»ºä¸€ä¸ªç›®å½•ï¼š `data` 
9. åˆ›å»ºä¸€ä¸ªæ–‡ä»¶ï¼š `data/run.sh` å¹¶å¡«å……ä¸‹é¢çš„å†…å®¹
*ä»Ž llama-gpt/api/run.sh å¤åˆ¶ï¼Œç„¶åŽè°ƒæ•´çš„*
```shell
#!/bin/bash

# make build

# Get the number of available threads on the system
n_threads=$(grep -c ^processor /proc/cpuinfo)

# Define context window
n_ctx=4096

# Offload everything to CPU
n_gpu_layers=37      

# Define batch size
n_batch=2096
# If total RAM is less than 8GB, set batch size to 1024
total_ram=$(cat /proc/meminfo | grep MemTotal | awk &#39;{print $2}&#39;)
if [ $total_ram -lt 8000000 ]; then
    n_batch=1024
fi

echo &#34;Initializing server with:&#34;
echo &#34;Batch size: $n_batch&#34;
echo &#34;Number of CPU threads: $n_threads&#34;
echo &#34;Number of GPU layers: $n_gpu_layers&#34;
echo &#34;Context window: $n_ctx&#34;

exec python3 -m llama_cpp.server --n_ctx $n_ctx --n_threads $n_threads --n_gpu_layers $n_gpu_layers --n_batch $n_batch
```  
   
10. è°ƒæ•´ `n_gpu_layers`å‚æ•°ï¼Œ è¿‡å¤§å¯èƒ½ä¼šå¯¼è‡´ OOMï¼Œ æˆ‘æ˜¯3060 12Gï¼Œ 37å·²ç»éžå¸¸æžé™äº†ã€‚
11. `sudo docker compose up`  æŸ¥çœ‹èƒ½å¦æ­£å¸¸è¿è¡Œï¼Œ å¦‚æžœæ²¡æœ‰ä»€ä¹ˆé—®é¢˜çš„è¯ï¼Œ åˆ™ä½¿ç”¨ `sudo docker compose up -d ` è¿›å…¥åŽå°è¿è¡Œã€‚


å…¶ä»–è¯´æ˜Ž
- GPU çš„å ç”¨å¥½åƒæ²¡æœ‰è·‘æ»¡ï¼Œ å¤§çº¦åœ¨ 30% é™„è¿‘ã€‚  CPUè¿˜æ˜¯100% åœ¨è¿è¡Œï¼Œ 
- é€Ÿåº¦ç¡®å®žæå‡äº†ä¸å°‘ï¼Œ ~~ä½†æ˜¯æ²¡æœ‰åˆ°è¾¾ç«‹å³å“åº”çš„åœ°æ­¥ã€‚~~  å¾ˆå¤šé—®é¢˜å¯ä»¥åœ¨å‡ ç§’å†…å¼€å§‹å›žç­”ã€‚
- èŠå¤©è®°å½•è¶Šå¤šï¼Œ è¶Šå®¹æ˜“ å´©æºƒï¼Œ æš‚æ—¶æ²¡çœ‹åˆ°å´©æºƒåŽŸå› ï¼Œåªçœ‹åˆ°äº† exit code 0
  - å¯èƒ½å’Œdocker å®¹å™¨å…³é—­å†å¯åŠ¨ä¹Ÿæœ‰ç‚¹å…³ç³»æŠŠã€‚
  - åº”è¯¥æ˜¯ åˆ°è¾¾äº† max_token  æˆ–è€…æ˜¾å­˜çˆ†äº†
- æ¨¡åž‹çš„è·¯å¾„å¥½åƒåªèƒ½ç”¨ `/models/llama-2-13b-chat.bin` ï¼Œæˆ–è€…åªèƒ½æ˜¯`/models/` ç›®å½•ï¼Œ å¦åˆ™ UI ç¨‹åºä¼šå‡ºçŽ°é—®é¢˜ã€‚



## å…¶ä»–

å°è¯•äº†ä¸€ä¸‹  https://github.com/oobabooga/text-generation-webui ï¼Œ åŒæ ·å®‰è£…æœ‰ç‚¹éº»çƒ¦ï¼Œ ä½†æ˜¯è¿™ä¸ªæŽ’ç‰ˆå¾ˆå¥½ã€‚ *ä½¿ç”¨çš„åŒä¸€ä¸ªæ¨¡åž‹*  
*2023å¹´9æœˆ12æ—¥ ä»Šæ—¥å°è¯•ï¼Œ æ— æ³•åŠ è½½è¿™ä¸ªæ¨¡åž‹äº†ã€‚*



---

> ä½œè€…: Aincvy  
> URL: https://fantasyplayer.link/periphery/llama-gpt-%E9%A1%B9%E7%9B%AE%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%88%87%E6%8D%A2cuda%E7%9A%84%E6%96%B9%E6%B3%95/  

